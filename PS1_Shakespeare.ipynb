{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS1-Shakespeare.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVt1PQphs7m5skWV92oxPe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mutherr/CS6120-PS1/blob/master/PS1_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVS67_HNRmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_validate,LeaveOneOut\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzjMY8fYQbB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read in the shakespeare corpus\n",
        "def readShakespeare():\n",
        "  raw = requests.get(\"https://raw.githubusercontent.com/mutherr/CS6120-PS1-data/master/shakespeare_plays.json\").text.strip()\n",
        "  corpus = [json.loads(line) for line in raw.split(\"\\n\")]\n",
        "\n",
        "  #remove histories from the data, as we're only working with tragedies and comedies\n",
        "  corpus = [entry for entry in corpus if entry[\"genre\"] != \"history\"]\n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039fPQcF7OkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here is where you will featurize the data.\n",
        "#NB: The current contents are for testing only\n",
        "#This function should return: \n",
        "#  -a numpy matrix of document features\n",
        "#  -a list of the correct genre for each document\n",
        "#  -a list of the vocabulary used by the features, such that the ith term of the\n",
        "#    list is the word whose counts appear in the ith column of the matrix. \n",
        "def createFeatures(corpus):\n",
        "  from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "  texts = [entry[\"text\"] for entry in corpus]\n",
        "  genres = [entry[\"genre\"] for entry in corpus]\n",
        "\n",
        "  vectorizer = CountVectorizer()\n",
        "  texts = vectorizer.fit_transform(texts)\n",
        "  vocab = vectorizer.get_feature_names()\n",
        "\n",
        "  return texts,genres,vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfTBqBltXe7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#given a numpy matrix representation of the features for the training set, the \n",
        "# vector of true classes for each example, and the vocabulary as described \n",
        "# above, this computes the accuracy of the model using leave one out cross \n",
        "# validation and reports the most indicative features for each class\n",
        "def evaluateModel(X,y,vocab,penalty=\"l1\"):\n",
        "  #create and fit the model\n",
        "  model = LogisticRegression(penalty=penalty,solver=\"liblinear\")\n",
        "  results = cross_validate(model,X,y,cv=LeaveOneOut())\n",
        "  \n",
        "  #determine the average accuracy\n",
        "  scores = results[\"test_score\"]\n",
        "  avg_score = sum(scores)/len(scores)\n",
        "  print(\"The model's average accuracy is %f\"%avg_score)\n",
        "  \n",
        "  #determine the most informative features\n",
        "  # this requires us to fit the model to everything, because we need a\n",
        "  # single model to draw coefficients from, rather than 26\n",
        "  model.fit(X,y)\n",
        "  neg_class_prob_sorted = model.coef_[0, :].argsort()\n",
        "  pos_class_prob_sorted = (-model.coef_[0, :]).argsort()\n",
        "\n",
        "  termsToTake = 20\n",
        "  pos_indicators = [vocab[i] for i in neg_class_prob_sorted[:termsToTake]]\n",
        "  neg_indicators = [vocab[i] for i in pos_class_prob_sorted[:termsToTake]]\n",
        "\n",
        "  print(\"The most informative terms for comedies are: %s\"%pos_indicators)\n",
        "  print(\"The most informative terms for tragedies are: %s\"%neg_indicators)\n",
        "\n",
        "def evaluateModelL2(X,y,vocab):\n",
        "  return evaluateModel(X,y,vocab,penalty=\"l2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IpJ7PKjvc8I",
        "colab_type": "code",
        "outputId": "4c14c0da-024d-44c3-baba-755639651b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        }
      },
      "source": [
        "#Run this to read the corpus and fit the model using your featurization scheme\n",
        "\n",
        "corpus = readShakespeare()\n",
        "\n",
        "X,y,vocab = createFeatures(corpus)\n",
        "\n",
        "#this call will fit a model with L1 normalization\n",
        "#print(\"----------L1 Norm-----------\")\n",
        "#evaluateModel(X,y,vocab)\n",
        "#this call will fit a model with L2 normalization\n",
        "print(\"----------L2 Norm-----------\")\n",
        "evaluateModelL2(X,y,vocab)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------L2 Norm-----------\n",
            "  (0, 499)\t1\n",
            "  (0, 19258)\t85\n",
            "  (0, 17221)\t345\n",
            "  (0, 5706)\t4\n",
            "  (0, 2342)\t97\n",
            "  (0, 19433)\t1\n",
            "  (0, 15298)\t1\n",
            "  (0, 5273)\t1\n",
            "  (0, 12546)\t1\n",
            "  (0, 9540)\t136\n",
            "  (0, 11735)\t476\n",
            "  (0, 7098)\t19\n",
            "  (0, 17225)\t708\n",
            "  (0, 5396)\t29\n",
            "  (0, 6809)\t21\n",
            "  (0, 1602)\t136\n",
            "  (0, 3840)\t40\n",
            "  (0, 14635)\t28\n",
            "  (0, 9649)\t118\n",
            "  (0, 596)\t58\n",
            "  (0, 11777)\t17\n",
            "  (0, 10166)\t209\n",
            "  (0, 12223)\t175\n",
            "  (0, 6877)\t1\n",
            "  (0, 17930)\t23\n",
            "  :\t:\n",
            "  (25, 18126)\t1\n",
            "  (25, 420)\t1\n",
            "  (25, 5582)\t1\n",
            "  (25, 628)\t1\n",
            "  (25, 1021)\t1\n",
            "  (25, 9445)\t1\n",
            "  (25, 14575)\t1\n",
            "  (25, 3677)\t1\n",
            "  (25, 14139)\t1\n",
            "  (25, 4902)\t1\n",
            "  (25, 1944)\t1\n",
            "  (25, 7111)\t1\n",
            "  (25, 7305)\t1\n",
            "  (25, 15622)\t1\n",
            "  (25, 2550)\t1\n",
            "  (25, 16752)\t1\n",
            "  (25, 2901)\t1\n",
            "  (25, 14662)\t1\n",
            "  (25, 11618)\t1\n",
            "  (25, 12383)\t1\n",
            "  (25, 12564)\t1\n",
            "  (25, 19463)\t1\n",
            "  (25, 6282)\t1\n",
            "  (25, 4820)\t1\n",
            "  (25, 9873)\t1 ['comedy', 'tragedy', 'comedy', 'comedy', 'tragedy', 'tragedy', 'tragedy', 'tragedy', 'tragedy', 'comedy', 'tragedy', 'comedy', 'comedy', 'comedy', 'comedy', 'comedy', 'tragedy', 'tragedy', 'comedy', 'comedy', 'tragedy', 'tragedy', 'tragedy', 'comedy', 'comedy', 'comedy']\n",
            "The model's average accuracy is 0.730769\n",
            "The most informative terms for comedies are: ['you', 'prospero', 'duke', 'helena', 'antonio', 'me', 'for', 'your', 'sir', 'ariel', 'sebastian', 'hermia', 'lysander', 'parolles', 'stephano', 'will', 'leontes', 'caliban', 'demetrius', 'love']\n",
            "The most informative terms for tragedies are: ['ham', 'iago', 'him', 'our', 'othello', 'what', 'his', 'lear', 'imogen', 'brutus', 'rom', 'nurse', 'romeo', 'caesar', 'thy', 'cassio', 'to', 'timon', 'posthumus', 'desdemona']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iHudrPb5NPY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}